import os, boto3, json
import pandas as pd
from typing import TypedDict, Annotated, List, Optional, Literal
from langchain_core.messages import BaseMessage, HumanMessage, AIMessage
from langchain_core.prompts import ChatPromptTemplate
from langgraph.graph import StateGraph, START, END
from langgraph.graph.message import add_messages
from langchain_aws import ChatBedrock  # or langchain_openai.ChatOpenAI, etc.
from dotenv import load_dotenv

# Environment setup
load_dotenv()
MODEL_ID = os.getenv("CHATBOT_LLM_ID")  # Your model ID
REGION = os.getenv("REGION")            # Your region (if AWS)
ALL_ROLES = ["TOP", "MIDDLE", "JUNGLE", "BOTTOM", "UTILITY"]
ALL_QUEUES = ["RANKED", "DRAFT", "BOTH", "AVERAGE_CHAMPION_X"]
USER_CHAMPION_SELECTION = ["MOST_PLAYED", "HIGHEST_WR", "MANUAL"]
EXPLORE_OR_OPTIMIZE = ["EXPLORATION", "OPTIMIZATION"]
OPTIMIZATION_SCOPE = ["CLUSTER", "ROLE"]

# LLM SETUP

bedrock = boto3.client("bedrock-runtime", region_name=os.getenv("AWS_REGION", "us-east-2"))

def call_llm(prompt: str) -> str:
    """
    Minimal call to Amazon Nova Micro using Bedrock's Converse API.
    Returns only the assembled text from the response.
    """
    resp = bedrock.converse(
        modelId=MODEL_ID,
        messages=[{"role": "user", "content": [{"text": prompt}]}],
        inferenceConfig={"maxTokens": 10, "temperature": 0.5},
    )
    # Extract plain text from output blocks
    blocks = resp.get("output", {}).get("message", {}).get("content", [])
    texts = [b.get("text", "") for b in blocks if isinstance(b, dict)]
    return " ".join(t for t in texts if t)


# 1) Define the shape of your state.
class State(TypedDict, total=False):
    reply: str
    foo: int
    seen: bool

# 2) A node that "looks at" the state (e.g., logs it) and optionally updates it.
def inspect_node(state: State) -> State:
    print(">>> Incoming state:", state)   # side-effect only
    return {"seen": True}                 # merge this into the state

def updated_state_node(state: State) -> State:
    return {"foo": state.get("foo") + 1}

def llm_node(state: State) -> State:
    try:
        text = call_llm(state.get("prompt","Say 'Hello, gamers!' exactly"))
        print(text)
        return {"reply": text}
    except Exception as e:
        return {"error": str(e)}

# 3) Build the tiniest possible graph.
graph = StateGraph(State)
graph.add_node("inspect", inspect_node)
graph.add_node("update", updated_state_node)
graph.add_node("llm", llm_node)
graph.add_edge(START, "inspect")
graph.add_edge("inspect", "update")
graph.add_edge("update", "llm")
graph.add_edge("llm", END)
app = graph.compile()
g = app.get_graph()
g.print_ascii()

# 4) Run once with any initial state you want.
if __name__ == "__main__":
    result = app.invoke({"foo": 42})
    print(">>> Final state:", result)